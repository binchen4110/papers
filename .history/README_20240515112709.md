# About

## Stay active in learning, one paper a day! 

Motivated by [Fredrik](https://www.fregu856.com/), I categorize, annotate and write comments for all research papers I read (4 papers since 2024). 


#### Categories:

[Uncertainty], [MultiModal], [MultiModal KGC], [TKG], [Multimodal Retrieval], [Contrastive Learning], [CV], [Image Edit], [Diffusion model]


### Papers:

- [Papers Read in 2024](#papers-read-in-2024)



#### Papers Read in 2024:



##### [2024-05-15] [paper6]
- From Local to Global: A Graph RAG Approach to Query-Focused Summarization [[pdf]](https://arxiv.org/pdf/2404.16130) [[annotated pdf]](https://github.com/binchen4110/reading_papers/blob/main/commented_pdfs/InstantID%20Zero-shot%20Identity-Preserving%20Generation%20in%20Seconds.pdf)
- *Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Jonathan Larson*
- `Arxiv 2024`
- [LLM] [RAG] [Graph] [Query-Focused Summarization
```
General comments on paper quality:
fairly novel; very very interesting; easy to read but the topic is unfamiliar to me; like from Lecun; 
```

```
Personal comment (in detail):
图像合成任务（ai换脸+风格迁移+图像编辑）；高效生成的同时，保存高保真度，就如题目中的"In Seconds"一样。核心部分为2个，一是Image Adapter，主要用于生成image prompt;二是IdentityNet，主要用于丰富面部信息如pose,表情等捕捉。

小红书出品，有Demo可以玩，效果还可以，挺有意思的。
```

##### [2024-05-14] [paper5]
- InstantID: Zero-shot Identity-Preserving Generation in Seconds [[pdf]](https://arxiv.org/pdf/2401.07519) [[annotated pdf]](https://github.com/binchen4110/reading_papers/blob/main/commented_pdfs/InstantID%20Zero-shot%20Identity-Preserving%20Generation%20in%20Seconds.pdf)
- *Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, Yao Hu*
- `Arxiv 2024`
- [CV] [Image Edit] [Diffusion model]
```
General comments on paper quality:
fairly novel; very very interesting; easy to read but the topic is unfamiliar to me; like from Lecun; 
```

```
Personal comment (in detail):
图像合成任务（ai换脸+风格迁移+图像编辑）；高效生成的同时，保存高保真度，就如题目中的"In Seconds"一样。核心部分为2个，一是Image Adapter，主要用于生成image prompt;二是IdentityNet，主要用于丰富面部信息如pose,表情等捕捉。

小红书出品，有Demo可以玩，效果还可以，挺有意思的。
```

##### [2024-05-13] [paper4]
- MyGO: Discrete Modality Information as Fine-Grained Tokens for Multi-modal Knowledge Graph Completion [[pdf]](https://arxiv.org/pdf/2404.09468v1) [[annotated pdf]](https://github.com/binchen4110/reading_papers/blob/main/commented_pdfs/MyGO%20Discrete%20Modality%20Information%20as%20Fine-Grained%20Tokens%20for.pdf)
- *Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Huajun Chen, Wen Zhang*
- `MM 2024`
- [Multimodal KGC] [Contrastive Learning]
```
General comments on paper quality:
fairly novel; the proposed "fine-grained" is interesting; easy to read;
```

```
Personal comment (in detail):
捕获更细粒度的多模态信息交互；代替直接编码各个模态信息，而是采用Tokenization，编码为token, 再使用transformer融合多模态多token的细粒度的信息，学习其交互；figure 5不太明白；此外，还采用了对比学习来提升表示学习。

（感悟：对比学习和GAN之类的对抗训练都可以提升表示，即可以互换）
```

##### [2024-05-12] [paper3]
- NativE: Multi-modal Knowledge Graph Completion in the Wild [[pdf]](https://www.techrxiv.org/doi/full/10.36227/techrxiv.171259566.60211714/v1) [[annotated pdf]](https://github.com/binchen4110/reading_papers/blob/main/commented_pdfs/NativE%20Multi-modal%20Knowledge%20Graph%20Completion%20in%20the%20Wild.pdf)
- *Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, Huajun Chen*
- `SIGIR 2024`
- [Multimodal KGC]
```
General comments on paper quality:
a little novel; the proposed "Wild" scenarios problem is interesting; easy to read;
```

```
Personal comment (in detail):
解决多模态KGC中的模态diversity和imbalanced 问题，设计了relation-guide的多模态融合机制；此外设计了GAN，对抗训练机制来提升表示学习和模型鲁棒性，并理论上证明了基于RotatE的score function满足K-Lipschitz约束，理论上证明了对抗训练机制的有用性；

时间效率的figure画法很有意思。总的来说，方法不是那么惊艳。
```

##### [2024-05-11] [paper2]
- Learnable Pillar-based Re-ranking for Image-Text Retrieval [[pdf]](https://arxiv.org/pdf/2304.12570) [[annotated pdf]](https://github.com/binchen4110/reading_papers/blob/main/commented_pdfs/Learnable%20Pillar-based%20Re-ranking%20for%20Image-Text%20Retrieval.pdf)
- *Leigang Qu, Meng Liu, Wenjie Wang, Zhedong Zheng, Liqiang Nie, Tat-Seng Chua*
- `SIGIR 2023`
- [Multimodal] [Multimodal Retrieval]
```
General comments on paper quality:
fairly novel; the proposed "Pillar" method is interesting; a little complex to read;
```

```
Personal comment (in detail):
Image2text的检索任务；在re-ranking阶段，使用Image的跨模态和本模态的邻居为基底（pillar）重定义特征表示，并使用两层变化的GCN学习表示，其中邻接矩阵的部分来源于对Image表示self-attention的相似度计算，文中称之为自学习的部分。

有点变换坐标轴，重新换基的味道。
```

##### [2024-05-10] [paper1]
- Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization [[pdf]](https://arxiv.org/pdf/2211.07394v6) [[annotated pdf]](https://github.com/binchen4110/reading_papers/blob/main/commented_pdfs/Composed%20Image%20Retrieval%20with%20Text%20Feedback%20via%20Multi-grained%20Uncertainty%20Regularization.pdf)
- *Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, Tat-Seng Chua*
- `ICLR 2024`
- [Multimodal] [Multimodal Retrieval] [Uncertainty]
```
General comments on paper quality:
very novel; the proposed method is interesting; easy to read;
```

```
Personal comment (in detail):
多模态image retrieval; 建模coarse-grained 匹配，对目标添加高斯噪音，以建模匹配的不确定性，1对多。

思路很简单。
```
